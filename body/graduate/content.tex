% 基于区块链的物联网时序数据高效存储技术研究

\chapter{绪论}
\section{研究背景及意义}
近年来，物联网（Internet of Things，IoT）发展迅速，促进了农业、制造业、医疗保健、零售业和公用事业等众多应用的发展。
根据Gartner的分析~\cite{hung2017leading}，数十亿部署的物联网设备将产生泽字节（ZettaBytes, ZB）的数据。
随着IoT的发展趋势，可以预见，未来将有越来越多的数据集成到网络中。
对于如此大规模的数据，使用去中心化服务器（例如AWS IoT、阿里云等）来管理它会遇到单点故障等问题。
尽管分布式数据库可以避免单点故障，但由于数据安全性和不可篡改性较弱，在需要高度透明的情况下，它们很容易受到数据篡改攻击，例如物联网数据共享~\cite{chen2022blockchain}。

区块链以可追溯性和不变性为特征，可以解决集中式存储的单点故障问题和分布式数据库的恶意篡改问题。
它将数据存储在去中心化的账本中，并使用共识协议来解决平等节点之间的冲突，这种方式增强了安全性，提高了透明度，降低了成本，支持点对点协作。
尽管它具有巨大的潜力，但其较低的交易速度和较高的计算要求使其仅广泛应用于价值巨大但密度很低的领域，如金融服务、供应链管理等。

目前，有许多研究人员致力于提高基于区块链的存储系统的性能。
根据数据的存储位置，这些工作可以分为两类，即链上存储和链下存储。

对于链上存储，数据作为存储在区块链上的交易记录的一部分，用户通过区块链中的索引，即默克尔树（Merkle Tree，MT）获取这些数据。
现有工作通过提供用户友好的查询语言~\cite{zhu2019sebdb,xu2019vchain,wang2022vchain+}提高了系统可用性，并通过改进索引方案~\cite{li2023lvmt,zhang2024cole}、区块链存储分片~\cite{zamani2018rapidchain,hong2023gridb,el2019blockchaindb}等方式提高系统吞吐量。
然而，在链上存储物联网数据的开销非常高。
对于大量快速生成的物联网数据，将其连续存储在区块链上需要实现共识和分类账复制的过程，这可能会导致巨大的存储压力和通信开销。
因此，物联网数据存储的一种更实用的方法是利用链下存储解决方案。

对于链下存储，数据存储在区块链之外，区块链只存储必要的元数据或对数据的引用，如哈希或加密指针。
链下存储提供了比链上解决方案更大的可扩展性和更低的成本，因此业界和学术界对链下存储非常有兴趣，例如Storj~\cite{storj2018storj}、BigchainDB~\cite{mcconaghy2016bigchaindb}、Sia~\cite{sia}等。
然而，现有的工作大多是为存储大文件而设计的。
对于单位粒度较小的物联网数据，考虑到物联网的海量大小，在区块链中存储每个数据项的哈希值将产生难以接受的开销。
此外，物联网应用场景通常需要存储系统支持高效查询（例如聚合查询），而现有的基于文件的存储系统不支持这一点。

\section{论文主要工作}
为了对海量物联网数据进行可信、高效的存储，本文提出了一种基于区块链的物联网时序数据高效存储技术，TimeChain。
受链下存储方案的启发，TimeChain对离散时间序列数据进行批处理，仅将每批数据的哈希值存储在链上，并将完整的原始数据保存在链外。
这种批处理存储方法大大减少了数据开销。
我们对该方案的性能进行了测量。
根据我们的测量，与单个数据存储相比，该方案的存储延迟平均减少了37.4倍。
这种存储性能使基于区块链的时间序列数据存储成为可能。

然而，不可否认的是，这样的批处理方案会影响查询性能。
具体来说，当用户执行范围查询时，低效的聚合方法会导致在多个传输节点上获取数据，从而导致额外的传输延迟。
此外，由于区块链上只存储了批处理单元的哈希值，因此存储系统必须传输额外的信息（例如默克尔树的哈希路径），以支持数据拥有者执行数据完整性验证。
这进一步增加了查询开销。

为了减少范围查询过程中跨越的存储节点数量，我们提出了一种新的自适应打包机制。
通过从原始数据构建无向加权图，我们将批处理问题转化为图划分问题。
我们使用谱聚类算法通过将频繁查询的数据分组在一起来解决分区问题，以减少聚合查询期间的节点访问次数。

为了在确保系统安全的同时进一步减少查询延迟，我们提出了一种基于共识的存储节点选择机制。
在选择节点时，我们基于之前的测量，受已有方案的启发，同时考虑存储节点信誉、剩余存储空间和传输距离。
为了在基于区块链的存储系统中快速做出节点选择决策，我们将共识过程与节点相结合，以减少传播延迟。

为了减少验证过程中的传输开销，我们提出了一种基于局部敏感哈希（Locality Sensitive Hashing，LSH）树的数据完整性验证机制。
该机制基于物联网场景下相邻时间序列数据点之间的相似性，通过仅传输证明的非冗余部分显著减少了完整性验证所需的数据。

我们基于开源组件（如Hyperledger Fabric和IPFS）实现了TimeChain，并基于此评估TimeChain的性能。
结果表明，与现有的基于区块链的存储系统相比，TimeChain平均减少了64.6\%的查询延迟和35.3\%的存储延迟。

\section{全文结构}
本文围绕基于区块链的物联网时序数据高效可信存储技术研究展开，其余部分组织如下：
第二章介绍了相关工作，包括分布式存储系统、基于区块链的存储系统和基于区块链的文件系统；
第三章介绍了目前基于区块链的物联网存储系统面临的需求和挑战，并提出了一种高效的物联网数据链下存储框架TimeChain；
第四章介绍了TimeChain的自适应聚合机制；
第五章介绍了TimeChain的存储节点选择机制；
第六章介绍了TimeChain的数据验证机制；
第七章介绍了TimeChain的性能评估；
第八章总结了全文并展望未来的工作。

\section{本章小结}
本章首先对本文的研究背景进行了介绍，阐明了边缘计算系统定制化的原因，介绍了系统构建的四个阶段并对现有问题进行了分析，从而引出本文选题的重要性。
在此基础上，对本文的主要研究工作进行了总结。

\chapter{相关工作}
\section{面向物联网的分布式存储系统}
近年来，随着数据规模的急剧增长和应用需求的多样化，集中式数据存储解决方案的固有漏洞逐渐显露出来，尤其是对于易受单点故障影响的局限性，引起了人们对分布式数据库的日益关注。
在传统集中式数据存储方式中，数据通常集中存储在单一节点或少数几个节点上，这种架构容易导致单点故障问题。
一旦出现故障，整个系统的稳定性和可用性将受到严重影响，甚至导致数据的不可靠或不可用。
相比之下，分布式数据库将数据分散存储在多个节点或数据中心中，这使系统本身就具备了抗单点故障的能力。
即使某个节点或数据中心发生故障，系统仍然可以继续运行并提供服务，从而保障了数据的可靠性和稳定性。
因此，随着对数据安全性、可用性和性能要求的不断提高，分布式数据库作为一种能够克服集中式存储固有弱点的解决方案，逐渐成为了当今大数据时代的主流选择。

Apache Cassandra~\cite{lakshman2010cassandra}作为一个分布式结构化键值存储系统，借鉴了Dynamo的分布式系统技术和Google BigTable的数据模型优势，通过特殊的数据分发策略确保数据存储的高可用性和容错性。
其灵活的数据模型和分布式特性使得它适用于需要横向扩展和高性能的应用场景。

Spanner~\cite{corbett2013spanner}是谷歌开发的全球分布式数据库。
它通过TrueTime API提供的时间戳服务实现全球性一致性和高可用性，并结合关系型数据库的ACID属性和分布式数据库的横向扩展，适用于需要高度一致性和水平扩展的工业应用场景。

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{timechain/ceph.pdf}
    \caption{Ceph存储流程}
    \label{fig:ceph}
\end{figure}

Ceph~\cite{weil2006ceph}作为一个开源的分布式存储系统，提供对象存储、块存储和文件系统存储功能。
为了平衡各存储节点的压力，它提出了可扩展哈希下的受控副本（Controlled Replication Under Scalable Hashing，CRUSH）算法，用于数据的分布式存储和数据冗余备份。
如~\autoref{fig:ceph}所示，对于需要被存储的文件而言，Ceph会首先将文件划分成多个独立的对象，然后Ceph将对象根据哈希分成若干放置组（Placement Group，PG）。
随后，CRUSH算法结合地理位置信息将数据对象映射到存储集群中的存储节点，实现数据的均衡分布和高效访问。
这种分布式、无中心化的数据分布方式使Ceph系统具有高度的可扩展性和容错性，即使在节点故障或网络分区的情况下，系统仍能保持数据的可靠性和可用性。

CockroachDB~\cite{taft2020cockroachdb}在Spanner的基础上，通过设计结合地理信息的复制和自动恢复机制来实现高容错性。
它将数据分布在不同的地理位置或数据中心，确保数据的冗余备份分布在不同的地理区域。
这种地理信息复制策略使得即使某个地理区域发生故障，数据仍然可以从其他地理位置的备份节点进行恢复。

然而，这些分布式存储系统仍然存在着单点故障和高存储成本的问题。
这是因为即使数据存储在不同位置，数据的管理仍然是中心化的。
在这些存储系统中，公司有可能在数据拥有者不知情的情况下轻松篡改数据，直接危及数据的完整性和安全性。
这些缺陷限制了这些系统在处理物联网数据激增和复杂协作需求方面的灵活性和效率，同时也使得数据的安全性和可靠性备受质疑。
因此，解决这些问题对于推动分布式存储系统在未来应用中获得广泛应用至关重要。

\section{基于区块链的存储系统}

\subsection{面向交易数据的链上存储}
区块链技术的出现为分布式存储系统的安全性和可靠性提供了新的解决方案。
区块链存储系统是一种基于区块链技术的分布式存储解决方案，其核心概念是将数据分布式存储在网络中的多个节点上，并使用区块链作为数据的验证和记录机制。
在区块链存储系统中，数据被分割成块，并以链式连接的方式进行存储，每个数据块包含了前一个块的哈希值，从而构成了不可篡改的数据记录链。
这种数据结构使得数据的完整性和安全性得到了极大的保障，任何尝试篡改数据的行为都会被系统所记录和拒绝。
区块链存储系统的去中心化特性意味着数据的管理不再依赖于单一实体，而是由网络中的多个节点共同维护和验证。
这种设计使得数据拥有者可以更加信任系统，因为数据的安全性和可靠性不再受限于单一实体的控制。
目前，许多链上存储的研究都集中在链上数据存储的性能上，包括查询性能和存储负担。

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{timechain/sebdb.pdf}
    \caption{SEBDB数据结构}
    \label{fig:sebdb}
\end{figure}

部分工作关注于增强链上存储的查询支持。
SEBDB~\cite{zhu2019sebdb}通过建立块内和块间索引来支持对区块链交易的类SQL查询，并且对于链上和链下的联合查询提出了优化算法。
MSTDB~\cite{zhou2022mstdb}则建立了基于语义的索引结构，支持基于语义的多关键字查询，并对于top-k查询以及范围查询的验证方法做出优化。

也有部分工作优化了区块上的索引和认证机制，以提高区块链状态的查询速度。
LVMT~\cite{li2023lvmt}采用了认证多点评估树（Authenticated Multipoint Evaluation Tree，AMT）在常数时间内更新完整性证明，并且采用多层设计来支持无限的键值对。
LVMT还通过存储版本号替代哈希的方式避免了昂贵的椭圆曲线乘法运算。
COLE~\cite{zhang2024cole}基于学习索引和列存储优化了区块链的存储索引结构。
COLE将数据按状态地址连续存储，通过学习索引模型减少存储索引开销；并针对区块链的存储磁盘特性引入了LSM树，以支持高效的读写性能。

部分工作关注链上存储的性能优化。
FalconDB~\cite{peng2020falcondb}和Chen等人提出的仲裁模型~\cite{chen2022blockchain}都关注区块链节点之间数据交互的效率。
为了减轻区块链账本数据存储的负担，Rapidchain~\cite{zamani2018rapidchain}、SlimChain~\cite{xu2021slimchain}和GriDB~\cite{hong2023gridb}将账本的数据分配给其他分片存储，以减轻存储压力。

在大规模数据生成迅速的物联网场景中，上述解决方案存在两方面的挑战。
首先，这些解决方案会给区块链带来额外的空间存储负担。
由于物联网设备在短时间内生成的数据量庞大，将这些数据全部存储在区块链上可能会导致链的存储需求急剧增加，进而增加系统的复杂性和成本。
其次，这些方案可能导致数据隐私泄露的风险增加，因为在区块链上存储的数据通常是不可篡改和公开可见的，这会导致物联网的数据泄露。
综上所述，在数据密度低、生成速度快的物联网场景下直接使用传统的链上存储方案，会产生较大的开销和风险。
物联网场景需要更加注重数据存储效率和隐私保护，因此需要针对这些特殊需求设计更为灵活和高效的数据管理方案。

\subsection{面向文件数据的链下存储}
基于区块链的文件系统因其较少的开销而备受广泛研究和关注。
这些系统将文件本身存储在链下，规避了原始数据直接存储在链上带来的巨大开销，并通过在区块链上记录文件的哈希值和验证信息来确保文件的完整性和安全性。
由于区块链的不可篡改性，用户可以方便地验证文件的来源和完整性，防止数据篡改和丢失。
相较于传统的中心化存储系统，基于区块链的文件系统通常不需要中心化的服务器和维护成本，从而降低了整体的运营成本。

Filecoin~\cite{bauer2022filecoin}是建立在星际文件系统（InterPlanetary File System，IPFS）的去中心化文件存储系统。
IPFS是一种点对点的分布式文件系统，它通过内容寻址来存储和共享文件，使得文件在没有单点故障的分布式网络上可用。
通过激励机制鼓励用户提供存储服务，Filecoin利用区块链技术确保了文件的完整性和可靠性，同时为用户提供了一种去中心化的存储解决方案。
在这个网络中，用户可以通过提供存储空间来获取Filecoin代币奖励，从而构建一个分布式的文件存储网络。
这种模式不仅激励了更多的存储资源加入网络，还提高了数据的持久性和可访问性，因为文件被分散存储在全球各地的节点上，减少了中心化存储的风险。

Storj~\cite{storj2018storj}和Sia~\cite{vorick2014sia}是两个典型的半去中心化的文件存储系统，它们为每个文件建立默克尔树以确保文件的完整性。
这种方法通过分散存储文件块来提高可靠性和安全性，从而降低了数据丢失的风险。
用户可以通过这些系统安全地存储和共享文件，同时保持数据的隐私和安全性。

FileDES~\cite{xu2024filedes}专注于数据的加密存储，借助零知识证明等技术来保护数据的安全性。
通过加密存储数据，FileDES可以确保用户的数据在存储和传输过程中不会被泄露或篡改。
这种方法为用户提供了更高级别的数据安全保障，尤其适用于对数据隐私和安全性要求较高的场景。

然而，尽管这些方法在确保文件完整性和安全性方面表现出色，它们并不适用于高效存储物联网数据的存储。
这是因为物联网数据通常具有较低的价值密度，而将单个数据以文件形式存储在区块链文件系统中，将导致极高的成本。
物联网数据的特点决定了需要更为轻量级和高效的数据存储方案，以更好地应对数据生成量大、频繁性高的特点，同时降低存储和处理成本。
因此，对于物联网大规模数据存储的需求，寻找轻量、高效、可信的数据解决方案是一个挑战。

\section{本章小结}
在本章中，我们广泛探讨了面向物联网的分布式存储系统和基于区块链的存储系统的相关研究工作。
我们分析了集中式数据存储解决方案的局限性，特别是它们在面对单点故障时的脆弱性，以及分布式数据库如何通过在多个节点间分散存储数据来提高数据的可靠性和稳定性。
我们详细讨论了Apache Cassandra、Spanner、Ceph和CockroachDB等分布式存储系统，它们通过不同的技术手段实现了数据的高可用性和容错性。
然而，这些系统仍然存在单点故障的风险，并且由于数据管理的中心化，数据的完整性和安全性仍然受到威胁。

进一步地，我们探讨了基于区块链的存储系统，它们通过将数据分布式存储在多个节点上，并利用区块链作为数据验证和记录的机制，提供了一种新的数据存储解决方案。
我们分析了链上存储和链下存储两种方法，以及它们在提高数据存储性能和确保数据完整性方面的研究进展。
尽管这些基于区块链的存储系统在确保数据完整性和安全性方面具有优势，但它们在处理大规模、低价值密度的物联网数据时面临效率和成本的挑战。

综上所述，现有的分布式存储系统和基于区块链的存储系统虽然在某些方面取得了进展，但它们在满足物联网数据存储需求方面仍存在不足。
这些挑战包括处理大规模数据的效率、数据存储的成本以及数据隐私保护的需求。
因此，开发一种轻量级、高效且可信的数据存储解决方案对于物联网领域来说是一个重要的研究方向。
TimeChain系统正是在这样的背景下应运而生，旨在结合区块链技术的优势，解决物联网时序数据存储中的关键问题。

\chapter{基于区块链存储系统的测量与研究}
\label{sec:baseline}
在本章中，受链下存储启发，我们先提出了一种基于区块链的物联网时序数据基本存储系统。
对于这个基本系统，我们进行了一些性能测试，并针对性能瓶颈，提出了高效链下存储系统的架构。

\section{基于区块链存储系统的架构}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{timechain/measurement_workflow.pdf}
    \caption{基于区块链的存储系统的工作流}
    \label{fig:workflow}
\end{figure}

为了提高基于区块链的分布式数据库的性能，我们提出了一种基本的链下存储系统，并对其进行了测量研究。
如~\autoref{fig:workflow}所示，一个基于区块链的基本分布式存储系统有四个主要角色，即数据拥有者、网关、卫星和存储提供商。
数据拥有者请求存储资源并查询数据。
网关为数据拥有者提供了一个与网络交互的接口，允许他们上传、下载和管理数据。
与Storj和FileCoin类似，卫星这一角色的功能是协调用户和存储提供商之间的沟通。
它们提供文件审计（Audit）或可检索性（Proofs of Retrievability，POR）相关功能和对存储支付等操作的处理。
为了确保交互过程的安全性，卫星通常以智能合约的形式运行。
存储提供商提供存储和检索数据的服务，通过提供服务获得奖励。
为了使存储提供商能够通过灵活的查询快速向数据拥有者提供完整性证明，数据完整性证明也需要存储在存储提供商中，便于通过默克尔树的路径直接快速组装成完整性证明。
存储提供商的服务信息，如剩余存储空间，将与交互记录一起记录在分布式账本中，以确保安全。
一般来说，数据存储和查询过程可以概括如下：

\textbf{数据存储：}
\ding{182}\textit{上传}：
数据拥有者通过网关接口上传数据。
\ding{183}\textit{聚合}：
网关对时间序列数据进行批处理，并生成每批数据的数据完整性证明。
\ding{184}\textit{选择存储节点}：
卫星帮助网关发现用于存储数据的最佳存储节点。
\ding{185}\textit{发送数据和完整性证明}：
原始传感器数据和完整性证明被发送到最佳存储提供商。
数据批的元数据记录在分布式账本中。

\textbf{数据查询：}
\ding{172}\textit{检索}：
数据拥有者请求下载他们的数据，网关与卫星交互以检索相应存储提供商的位置。
\ding{173}\textit{获取数据和完整性证明}：
网关从存储提供商处获取数据和完整性证明。
\ding{174}\textit{验证}：
网关通过检查数据完整性证明来验证下载数据的完整性。
\ding{175}\textit{返回}：
网关将数据返回给数据拥有者。

\section{基于区块链存储系统的测量}
在本节中，我们进行了一项初步测量，以评估基于区块链的基本存储系统的性能。
我们基于Hyperledger Fabric实现了上述存储系统。
该测试网络由5个节点组成，其中1个节点是网关，4个节点是卫星。
我们基于真实时间的数据存储网络模拟了一个包括320个存储节点的网络，其中包括189个真实存储提供商~\cite{corneo2021surrounded}和131个模拟提供商（代表提供闲置存储的个人提供商）。
网关与模拟提供商的传输延迟通过现有的线性回归模型~\cite{ziviani2005improving}得到。
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.85\linewidth}
	    \centering
        \subfloat[存储时延]{
            \centering
            \includegraphics[width=0.45\textwidth]{timechain/measurement_storage.pdf}
            \label{fig:measurement_storage}
        }
        \hfill
        \subfloat[查询时延]{
            \centering
            \includegraphics[width=0.45\textwidth]{timechain/measurement_query.pdf}
            \label{fig:measurement_query}
        }
        \caption{区块链存储系统的性能} 
    \end{minipage}
\end{figure*}

\textbf{存储性能。}
我们设置数据拥有者每秒生成20个56字节的数据包，并在20秒内存储它们。
存储性能结果如图~\autoref{fig:measurement_storage}所示。
与单独存储每个数据相比，批存储将延迟减少了约37.4倍。
这主要是因为较大的批处理规模减少了链上交易的数量。

\textbf{查询性能。}
然后，我们测试范围查询的性能，如图~\autoref{fig:measurement_query}所示。
不幸的是，结果显示，批处理存储解决方案的查询性能相对较差，在不同批处理大小下的平均延迟为165.4ms，无法满足许多物联网场景的需求。
例如，自动驾驶应用程序的延迟小于50ms~\cite{caesar2020nuscenes}，地震监测的延迟小于100ms~\cite{bhatia2023artificial}。

\section{性能瓶颈的根本原因}
为了找出该方案查询性能不佳的原因，我们进行了深入研究，并将原因总结为以下三点：

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.8\linewidth}
	    \centering
        \subfloat[查询跨越多个batch]{
            \centering
            \includegraphics[width=0.3\textwidth]{timechain/batch_cdf.pdf}
            \label{fig:batch_cdf}
        }
        \hfill
        \subfloat[不正确的存储节点选择]{
            \centering
            \includegraphics[width=0.3\textwidth]{timechain/dis_cdf.pdf}
            \label{fig:dis_cdf}
        }
        \hfill
        \subfloat[传输数据较大]{
            \centering
            \includegraphics[width=0.3\textwidth]{timechain/proof_size_cdf.pdf}
            \label{fig:proof_size_cdf}
        }
        \caption{性能低下的根本原因} 
    \end{minipage}
\end{figure*}

\textbf{1）范围查询跨越多个批处理单元。}
时间序列数据的查询通常包含多个数据点，如范围查询、聚合查询、过滤查询等。
如果批处理不当，单个查询可能会跨越多个批处理单元。
我们使用现有的数据集YCSB~\cite{barata2014ycsb}评估每个查询所跨越的批处理单元数量。
如图~\autoref{fig:batch_cdf}所示，超过84.25\%的查询跨越了10个批处理单元。
当这些批处理单元被存储在不同的节点上时，将引入额外的查询和传输延迟。

\textbf{2）存储节点选择不当。}
在这个测量中，我们发现传输延迟占总查询延迟的很大一部分。
如图~\autoref{fig:dis_cdf}所示，世界各地存储节点的距离存在很大差异，导致节点之间的传输延迟存在很大差异。
如果选择非常远的存储节点，传输延迟将会显著增加。
此外，在整个存储网络中存在恶意节点的情况下，尽管最终选择的节点可能传输延迟较小，然而，由于恶意节点的存在，数据的完整性可能会受到威胁，从而无法提供完整的数据存储服务。

\textbf{3）完整性证明数据量大。}
为了使存储提供商能够通过灵活的查询快速向数据拥有者提供完整性证明，数据完整性证明也需要与存储提供商一起存储。
因此，当存储提供商需要向数据拥有者证明数据的完整性时，完整性证明也会被发送回数据拥有者。
图~\autoref{fig:proof_size_cdf}显示了总传输数据量的各组成部分。
从图中可以看出，数据完整性证明大小占接收数据的48.8\%，几乎是接收数据的一半。
当网络繁忙时，大量的完整性证明数据传输会增加网络传输延迟。

\section{高效的链下存储架构}
\label{sec:design}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{timechain/arch.pdf}
    \caption{TimeChain架构}
    \label{fig:architecture}
\end{figure}

为了提高区块链存储系统的查询性能，我们设计了一种基于区块链的新型物联网时间序列数据存储系统，TimeChain。
\autoref{fig:architecture}显示了TimeChain的架构。
TimeChain建立在区块链平台上，所有操作都记录在分布式账本上。
TimeChain中的核心模块包括数据批处理、存储提供商选择和数据验证。
TimeChain的索引结构是R树，可以加速物联网场景中常用的时空聚合搜索。
接下来，我们介绍TimeChain的关键模块。

\textbf{数据批处理模块：}
我们的测量研究表明，不正确的聚合方法会增加查询的跨单元数量，从而增加网络传输延迟。
我们构建了一个自适应的无向加权图（Undirected Weighted Graph，UWG），以基于数据拥有者的历史查询准确捕获用户查询信息（§-\ref{sec:UWG}）。
基于我们构建的UWG，我们将数据打包成批处理单元的问题转换为聚类问题~\cite{xu2005survey}，该问题根据用户的查询请求将所有原始数据划分为多个聚类。
解决聚类问题的传统算法有很多，如K-means~\cite{kanungo2002efficient}、GMM~\cite{he2010laplacian}等。
然而，这种传统的聚类算法不适合分割物联网设备生成的数据。
这是因为在TimeChain中，用户的查询没有遵循特定的特征，这可能会导致数据图的聚类形成复杂的形状，而不是常见的圆形。
此外，传统的聚类算法需要将所有数据划分为固定数量的集合，但并非所有集合都等于批大小，这将给索引查询带来额外的开销。
因此，我们使用谱聚类算法来打包数据（§-\ref{sec:ratiocut}），这非常适合处理不规则和非固定数量的聚类。

\textbf{存储节点选择模块：}
存储节点的\textit{选择}至关重要。
正如我们之前在图~\autoref{fig:dis_cdf}中发现的那样，存储节点和网关之间的距离会影响数据访问延迟。
此外，对于链下存储系统，存储空间不足的节点或恶意节点可能会导致数据丢失、篡改或服务中断，进而影响整个系统的安全性和稳定性。
因此，我们根据距离和历史服务记录等信息对存储节点进行综合评估。
存储节点\textit{选择过程的安全性}也非常重要。
Storj~\cite{storj2018storj}、CoopEdge~\cite{yuan2021coopedge}和PipeEdge~\cite{yuan2023pipeedge}通过一组固定的节点选择服务节点，并通过区块链确认决策。
换句话说，他们集中决策，但仍然面临单点失败的威胁。
然而，使用类似于PBFT的投票机制，节点选择过程通常需要多轮任务计算和消息广播。
如果共识过程和节点选择过程完全解耦，系统安全将受到威胁。
为了解决这个问题，我们将节点选择过程与共识相结合，提出了一种基于共识的节点选择机制（§-\ref{sec:consensus}）。

\textbf{数据验证模块：}
从之前的测量结果中我们可以发现，传输的数据中接近一半是数据完整性证明。
数据完整性证明被组织为默克尔树的形式，它是由一系列哈希构建的。
在默克尔树中，非叶子节点的哈希数几乎等于原始数据点的数量。
由于物联网数据单元的大小大约等于哈希值，这意味着需要发送以验证数据的数据量几乎是原始数据的两倍。
在这样的数据传输过程中，减小数据证明的大小是一个挑战。
通过分析物联网数据，我们观察到物联网数据变化缓慢，在短时间内很少出现突然变化。
对于这些相似的数据，LSH算法可以从相似的原始数据中生成相似的哈希结果。
LSH算法确保类似的物联网数据即使在哈希后也保持相似。
因此，我们提出了一种新的基于LSH树的验证机制（§-\ref{sec:lsh}），该机制采用LSH而不是传统默克尔树中使用的通用哈希。
通过差分传输LSH哈希值，可以显著减小传输数据的大小。

\section{本章小结}
在本章中，我们深入探讨了基于区块链的物联网时序数据存储系统的基础架构，并对该系统进行了全面的测量研究。
我们的初衷是设计一种高效的链下存储方案，以应对物联网数据的快速增长和对数据安全性的严格要求。
通过对基本存储系统的性能测试，我们发现批处理存储能够显著降低存储延迟，但同时也带来了查询延迟的问题，特别是在执行范围查询等操作时。

我们进一步分析了导致查询性能不佳的根本原因，包括查询跨越多个批处理单元、存储节点选择不当以及庞大的数据完整性证明。
这些发现为我们提供了针对性能瓶颈优化的机会。

为了解决这些问题，我们提出了TimeChain架构，这是一种新型的基于区块链的物联网时间序列数据存储系统。
TimeChain的核心优势在于其数据批处理模块、存储节点选择模块和数据验证模块。
这些模块共同作用，旨在减少查询延迟、提高数据存储的效率和安全性。

具体来说，数据批处理模块通过构建自适应无向加权图和采用谱聚类算法来优化数据打包过程。
存储节点选择模块则通过结合共识机制来确保节点选择的安全性和准确性。
最后，数据验证模块通过引入基于LSH树的验证机制，有效减少了数据传输的大小，从而降低了网络传输延迟。

综上所述，本章不仅展示了基于区块链存储系统的性能特点，还提出了针对性能瓶颈的解决方案，并详细介绍了TimeChain系统的设计。
我们将在后续的三个章节中介绍TimeChain的具体模块和所使用的技术。

\chapter{面向链下存储的自适应聚合机制}
\label{sec:packaging}
我们的实验结果揭示了一个关键问题：不当的数据聚合策略会导致查询过程中需要跨越更多存储单元，进而增加了网络传输的延迟。
在本章中，我们提出了一种自适应UWG的方法，它能够根据数据所有者的历史查询模式精确捕捉用户的查询行为。
利用这个自适应UWG，我们将数据分批的问题转化为一个聚类问题，即根据用户的查询需求将海量原始数据划分成多个有意义的聚类。

尽管存在多种传统的聚类算法，例如K-means和GMM，它们不适合处理物联网设备产生的数据时。
原因在于TimeChain系统中用户查询的模式多样化，不遵循统一特征，这可能导致聚类结果呈现出不规则的形状，而非典型的圆形。
此外，这些传统算法通常要求将数据均匀分配到固定数量的聚类中，这与我们的需求不符，因为不是所有的聚类都恰好匹配一个批次的大小，这无疑会增加索引查询的负担。

\section{基于历史查询的自适应图构建过程}
\label{sec:UWG}

% \begin{table}
%     \centering
%     \caption{符号表}
%     \begin{tblr}{
%       column{1} = {c},
%       hline{1,13} = {-}{0.08em},
%       hline{2} = {-}{0.05em},
%     }
%     \textbf{Symbol} & \textbf{Description}\\
%         $d_i$   & 第$i$个设备 \\
%         $D$     & 所有设备集合，$D = \{d_0, ..., d_n\}$\\
%         $s_i$   & 物联网传感器产生的第$i$条数据 \\
%         $S$     & 所有数据集合，$S = \{s_0, ..., s_m \}$\\
%         $Q^i$   & 第$i$条用户查询集合，$Q^i = \{ q_0, ..., q_r \}$\\
%         $q_k$   & $k$中的第$Q^i$条查询 \\
%         $l_{ab}$& 查询权重包含设备$a$和设备$b$ \\
%         $L$     & 一起查询的数据权重，$L = \{l_{ab} | \exists_{a,b} \in D \}$\\
%         $x^k_{ab}$ & 设备$a$和设备$b$是否同时被查询 \\
%         $X^k$   & 设备访问信息，$X^k = \{x^k_{ab} | \exists_{a,b} \in D \}$\\
%         $P$     & 数据打包结果，$P = \{ \{ s_i, ... \}, ... \}$
%     \end{tblr}
%     \label{tab:notations}
% \end{table}

由于物联网的原始数据是孤立的点，我们在数据点之间创建加权边，表示被联合查询的概率。
数据点$a$和$b$之间的边的权重$l_{ab}$表示为：

\begin{equation} 
    \label{eq:weight}
    \begin{split}
        l_{ab} =
        \begin{cases}
            \sqrt{ (id_a - id_b)^2 + (t_a - t_b)^2 } &, k = 0 \\  
            \theta \cdot l_{ab} + (1 - \theta) \cdot x_{ab}^k &, k \geq 1  
        \end{cases}
    \end{split}
\end{equation}

其中$l_{ab}$被初始化为两个数据点设备ID和数据到达时间之间的欧氏距离。
当用户的请求到达时，UWG会根据请求中涉及的数据查询范围进行动态调整。
为了避免查询图的过度存储开销，我们在更新图时忽略了数据的时间维度，只考虑数据的设备ID维度。
我们使用变量$x_{ab}^k$来指示第$k$个查询的内容。
当第k个查询包含设备$d_a$和设备$d_b$时，$x_{ab}^k=1$，否则$x_{1b}^k=0$。
然后，距离$l_{ab}$将根据$x_{ab}^k$进行更新。
由于用户请求可能非常随机，因此图的更新和变化可能会非常频繁。
因此，我们设置了一个影响因子$\theta$来确定用户请求对图更新的影响。
当$\theta$接近1时，权重受查询的影响更大。
当$\theta$接近0时，这意味着图尽可能保持初始状态。
同时，TimeChain使用滑动窗口动态更新UWG，尽可能避免单个查询的对系统的鲁棒性影响。
虽然查询模式的突然变化最初会降低打包准确率，但系统会在多次查询后快速适应。
通过自适应权重聚类算法，我们可以根据节点之间的距离和查询的相关性动态调整节点之间的权重，以更好地反映它们的相似性。
这有助于在打包过程中更准确地确定哪些节点的数据应放置在同一批中，以提高查询的效率和准确性。

\section{基于谱聚类算法的封装机制}
\label{sec:ratiocut}

\begin{algorithm}[t]
	\caption{聚合算法}
	\label{algo:package}
    \begin{algorithmic}[1]
        \REQUIRE 输入设备集$D$，数据集$S$，用户的历史查询集$Q^i$
        \ENSURE 最优聚合结果$P$
        \STATE $S' \gets \{s^a | a \in D \And s^a \subset S\}$ //根据设备ID梳理原始数据
        \STATE $L \gets \Big\{ \sqrt{ (id_a - id_b)^2 + (t_a - t_b)^2 } \Big| \exists_{a,b \in D} \Big\}$ //根据\autoref{eq:weight}初始化权重集$L$
        \STATE $X^k \gets \{0 | \exists_{a,b \in D} \}$ //将用户查询请求信息$Q^{i-1}$打包成集合$X^k$
        \FOR{$q^k \in Q^{i}$}
            \IF{$a,b \in q^k$}
                \STATE \textnormal{使用$x^k_{ab} \gets 1 $更新$X^k$} //将用户查询请求信息$Q^{i-1}$打包成集合$X^k$
            \ENDIF
        \ENDFOR
        \STATE $L \gets \Big\{ \theta \cdot l_{ab} + (1 - \theta) \cdot x_{ab}^k \Big| \exists_{a,b \in D} \exists_{l_{ab} \in L} \Big\}$ //更新UWG的权重集$L$
        \STATE $D' \gets \textit{cluster}(D, L)$ //
        \STATE $P \gets \{\}$ //
        \FOR{$d^j \in D'$}
            \STATE \textnormal{增加$\{ s^a | \exists_{a \in d^j} \}$到$P$中} //
        \ENDFOR
        \STATE \textbf{return} $P$
    \end{algorithmic}
\end{algorithm}

由于谱聚类算法适用于处理不规则形状的分类问题，我们使用它来聚合数据。
\ref{algo:package}显示TimeChain的整个打包过程。
该算法的输入包括输入设备集$D$、数据集$S$和用户的历史查询集$Q^i$。
我们首先根据设备ID和时间单位将原始数据$S$组织到一个集合$S'$中，该集合以时间单位表示设备的数据（第1行）。
我们根据\autoref{eq:weight}初始化权重集$L$（第2行）。
对于上一个滑动窗口中的历史查询记录集合$Q^{i-1}$，我们将用户查询请求信息打包成集合$X^k$（第4-8行）。
然后，我们根据收集到的查询信息$X^k$更新权重集$L$（第9行）。
对于UWG$(D,L)$，我们使用谱聚类算法来获得聚合结果$D'$（第10行）。
根据聚合结果$D'$，我们将设备数据合并到$P$中，这是我们得到的聚合结果（第12-14行）。

\section{本章小结}
在本章中，我们深入研究了面向链下存储的自适应聚合机制，旨在解决物联网时序数据存储中的查询延迟问题。
我们发现，不当的数据聚合策略会导致查询过程中需要跨越更多存储单元，从而增加了网络传输的延迟。
为此，我们提出了一种基于历史查询的自适应无向加权图（UWG）方法，该方法能够精确捕捉用户的查询行为，并根据这些行为将数据分批处理，以优化查询效率。

我们首先构建了一个自适应UWG，它根据数据所有者的历史查询模式动态调整数据点之间的权重，从而更准确地反映数据的相关性和查询的动态性。
这种方法将数据分批的问题转化为聚类问题，使得我们可以根据用户的查询需求将海量原始数据划分为多个有意义的聚类。

在聚类算法的选择上，我们发现传统的聚类算法如K-means和GMM并不适用于处理物联网设备产生的数据，因为这些数据的聚类结果往往呈现出不规则的形状，且传统算法要求将数据均匀分配到固定数量的聚类中，这与我们的需求不符。因此，我们采用了谱聚类算法来处理这种不规则和非固定数量的聚类问题。

我们详细描述了基于谱聚类算法的封装机制，并提供了相应的算法流程。该算法考虑了输入设备集、数据集和用户的历史查询集，通过一系列的步骤，包括数据的组织、权重集的初始化和更新，以及聚类结果的获取，最终实现了数据的有效聚合。

\chapter{基于共识协议的节点选择机制}
\label{sec:consensus}
确定最佳的存储节点选择是构建高效区块链存储系统的关键一环。
根据我们的分析，如图~\autoref{fig:dis_cdf}所示，存储节点与网关之间的距离对数据访问延迟有着直接的影响。
此外，在链下存储环境中，节点存储空间不足或恶意行为可能会导致数据的丢失、被篡改或服务中断，这些都会对系统的安全性和稳定性造成严重影响。
因此，我们综合考虑了节点间的距离和它们的历史服务记录，对存储节点进行了全面的评估。

同时，确保存储节点选择过程的安全性同样不可忽视。
现有的系统如Storj、CoopEdge和PipeEdge，尽管通过区块链来确认节点选择的决策，却依然依赖于集中式的节点选择服务，这使得它们仍然面临着单点故障的风险。
尽管采用类似PBFT的投票机制可以在一定程度上确保节点选择的安全性，但这通常涉及到多轮的计算和消息广播，增加了系统的复杂性。
如果共识形成过程与节点选择过程完全分离，那么系统的安全性可能会受到威胁。

为了克服这些挑战，我们采取了一种创新的方法，将节点选择过程与共识机制紧密耦合，从而提出了一种基于共识的节点选择机制。
这种方法不仅提高了节点选择的效率和安全性，而且增强了整个系统的抗攻击能力和可靠性。

\section{结合共识的节点选择过程}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{timechain/consensus.pdf}
    \caption{基于共识的节点选择机制工作流}
    \label{fig:consensus}
\end{figure}

\begin{algorithm}
	\caption{共识过程}
	\label{algo:consensus}
	\begin{algorithmic}[1]
        \renewcommand{\algorithmicrequire}{ \textbf{准备}}
        \REQUIRE
            \STATE $request \gets \textit{receive}(REQUEST)$
            \IF{$request$ 有效}
                \STATE $d_i \gets \textit{distance}(i.pos, request.pos)$
                \STATE $s_i \gets i.space$
                \STATE 通过\autoref{eq:score}计算$p_i$
                \STATE $\textit{broadcast}(p_i, \textit{PREPARE})$
            \ENDIF

        \renewcommand{\algorithmicrequire}{ \textbf{预提交}}
        \REQUIRE
            \STATE $\{p_i\} \gets \textit{receive}(\{p_i\}, \textit{PREPARE})$
            \IF{$\textit{count}(\textit{PREPARE}) > 2 * f + 1 \textit{ 且超时}$}
                \STATE $P \gets \max_n \{p_i\}$
                \STATE $\textit{broadcast}(P, \textit{PRECOMMIT})$
            \ENDIF

        \renewcommand{\algorithmicrequire}{ \textbf{提交}}
        \REQUIRE
            \STATE $P \gets \textit{receive}(P, \textit{PRECOMMIT})$
            \IF{$count(P) > f + 1$}
                \STATE $\textit{commit}(P)$
            \ENDIF
	\end{algorithmic}
\end{algorithm}

为了安全地选择最佳存储节点，TimeChain提出了一种基于共识机制的节点选择算法。
整个选择过程包括\textit{提交}、\textit{准备}、\textit{预提交}、\textit{提交}和\textit{回复}，其详细信息如~\autoref{fig:consensus}所示，算法如~\autoref{algo:consensus}所示。
与PBFT共识类似，在\textit{request}阶段，网关向系统中的所有节点发送请求，共识节点将在\textit{reply}阶段将获得的结果返回给网关。
同样类似于PBFT共识，我们假设拜占庭卫星的数量为$f$，卫星总数超过$3f+1$。

在\textit{准备}阶段，每个节点通过考虑存储节点的距离、信誉等来计算分数，并将分数广播给所有其他节点。
TimeChain选择距离、剩余存储和存储质量作为指标，原因如下：
根据图~\autoref{fig:dis_cdf}，我们发现存储节点的距离会影响数据访问延迟。
因此，我们选择距离作为一个重要的指标。
此外，受到FileCoin~\cite{bauer2022filecoin}的启发，剩余存储空间和存储质量也是选择存储节点的重要因素。
剩余存储空间可以直接影响存储提供商的服务能力，而存储质量可以影响数据的完整性。
第$i$个节点的得分计算公式如下：

\begin{equation} 
    \label{eq:score}
    p_i=\alpha\cdot d_i+\beta\cdot s_i+\gamma\cdot q_i
\end{equation}

其中$d_i$表示第$i$节点和客户端节点之间的距离；$q_i$表示存储服务质量，可以从链上的服务记录中评估；$s_i$表示节点的剩余存储空间。
所有这些数据都可以在链上找到。
$\alpha$、$\beta$和$\gamma$是加权参数，这些系数可以根据特定的系统需求和性能要求进行调整。
例如，可以增加的权重以优先考虑存储完整性，而可以调整的权重以降低延迟。

在\textit{预提交}阶段，共识节点从其他节点接收准备好的消息集$\{p_i\}$。
当此节点的计时器超时并且收到超过$2f+1$个PREPARE消息时，共识节点根据它们收到的信誉优先级$\{p_i\}$决定最佳存储提供者。
如果每一轮共识只返回最近的节点，由于距离对信誉计算机制的影响，一些较近节点的存储压力可能会非常高。
为了平衡负载，共识节点将返回一组信誉最高的$n$节点，供网关随机选择，而不是信誉最高的节点。

在\textit{提交}阶段，所有节点都将收到其他节点推荐的最佳存储决策。
当相同的预提交消息的数量超过$f+1$时，此节点将向客户端提交最佳存储节点。

\section{共识过程的安全性分析}
我们在这里考虑这一共识协议的安全性。
由于TimeChain的共识协议只是基于PBFT添加了额外的信息，我们只考虑\textit{prepare}和\textit{pre-commit}阶段额外信息带来的安全风险。

在\textit{prepare}阶段，如果一个节点伪造了自己的分数，网关可以很容易地检查分数的真实性，因为评估数据源都可以在链上找到。
一旦节点伪造了其信誉，该行为也将被记录在链上，从而影响下一次的信誉评估。
此外，由于最后只选择了一个存储节点，网关不关注所有节点得分的真实性，只关注所选节点的得分。

在\textit{pre-commit}阶段，如果任何节点伪造了最终得分，则不会影响最终结果。
这是因为对于包含$f$拜占庭节点的$3f+1$节点的网络，必须在\textit{commit}阶段获得相同的结果。

\section{本章小结}
在本章中，为了保障选择存储节点的质量和节点选择过程的安全性，我们将节点选择过程与共识过程结合，提出了基于共识的节点选择机制。
我们对提出的共识机制进行了安全性分析，表明我们在PBFT基础上增加的信息并没有引入额外的安全性风险，因此该机制是较安全的。

\chapter{基于LSH树的验证机制}
\label{sec:lsh}
我们的实验数据揭示了一个显著的现象：在传输的数据中，数据完整性证明几乎占据了一半的比例。
这些证明通常以默克尔树的形式组织，由一连串的哈希值构成。
在默克尔树结构中，非叶子节点的哈希数量与原始数据点的数量大致相同。
鉴于物联网数据单元的大小与哈希值相近，这意味着为了验证数据的完整性，需要传输的数据量几乎是原始数据量的两倍。
在数据传输过程中，如何减少数据证明的体积，成为了一个亟待解决的难题。

深入分析物联网数据的特性后，我们注意到物联网数据的变化通常较为缓慢，短期内不太可能发生剧烈变动。
针对这些高度相似的数据点，LSH算法能够从相似的原始数据中产生相似的哈希结果。
LSH算法的这一特性保证了即使在哈希处理之后，相似的物联网数据点仍然保持相似性。
基于这一观察，我们设计了一种创新的基于LSH树的验证机制，它采用LSH算法替代了传统默克尔树中使用的通用哈希函数。
通过仅传输LSH哈希值的差异部分，我们能够显著减少在验证过程中需要传输的数据量，从而提高了数据传输的效率。

\section{LSH树结构与工作流程}

\begin{figure}[t]
    \centering
	\begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{timechain/lsh-tree.pdf}
        \caption{LSH树结构}
        \label{fig:lsh_tree}
	\end{minipage}
\end{figure}

对位置敏感的哈希将类似的数据映射到类似的哈希值以进行重复数据删除。
顾名思义，我们通常打包在物理空间或时间上接近的数据，这些数据往往具有很高的局部相似性。
因此，通过在同一批上使用局部敏感哈希，我们可以得到相似的哈希值。
当需要传输哈希值时，只传输哈希差部分，从而减少了要传输的数据量。

我们在~\autoref{fig:lsh_tree}中展示了LSH树的一个例子。
具体来说，对于批处理中的数据，我们采取类似于默克尔树的步骤，首先对原始数据执行局部敏感哈希。
对于哈希结果，我们将两个相近的哈希合并为一个字符串，并计算该字符串的局部敏感哈希值。
然后，我们逐层向上重复这个过程，直到我们得到一个唯一的哈希值，即哈希根。
在第一级哈希中，由于原始数据的高度相似性，哈希值的许多位是相同的。
因此，在传输第一级哈希值时，我们只能传输不同的比特以减少传输延迟。
同样，由于第一级哈希中存在局部相似性，第二级哈希中的许多比特也将相似。
通过类比，对于每一层的哈希值，我们只需要传输哈希差比特，从而进一步减少了传输的数据量。

由于我们使用LSH树而不是原始的默克尔树，因此我们需要分析LSH树的安全性。
这里我们主要考虑存储提供者篡改数据的情况，即仍然可以用不同的原始数据获得相同的哈希值的情况。
我们测试了LSH树的不同层，发现在最接近数据源的哈希层中，哈希的平均差位数为170位。
这已经超过了MD5和SHA1标准，这些标准现在在物联网场景中非常常用~\cite{chi2017hashing,landge2018secured}。
对于靠近根节点的LSH树的层级，尽管哈希差位数较少，但这对篡改原始数据没有意义。

\section{LSH树的尾部合并机制}

\begin{figure}[t]
    \centering
	\begin{minipage}{0.6\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{timechain/tail_merging_lsh.pdf}
        \caption{非满二叉LSH树}
        \label{fig:tail_merging_lsh}
	\end{minipage}
\end{figure}

\begin{figure}[t]
    \centering
	\begin{minipage}{0.6\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{timechain/tail_merging_tail.pdf}
	\end{minipage}
	\caption{尾部合并下的非满二叉LSH树.}
	\label{fig:tail_merging_tail}
\end{figure}

在一个完整的二叉树中，LSH树可以通过成对地批量合并数据来执行局部敏感哈希。
然而，如果批处理中的数据数量不足以形成完整的二叉树，那么像默克尔树一样构建哈希树将导致相似性的丧失。
例如，在~\autoref{fig:tail_merging_lsh}中，一批中有7个数据点，对于一个完整的二叉树来说，这个大小是不满足的。
在第一轮哈希中，前6个数据成对执行LSH。
由于原始数据的相似性，这6个数据的哈希结果是相似的。
在第二轮哈希中，由于数据5-6和第7个原始数据的第一轮哈希结果非常不同，这两个数据的哈希结果也与数据点1-4的哈希结果非常不一样。
在执行完整性证明时，需要传输所有这些不同的哈希数据位，这增加了传输的数据量。

为了解决这个问题，我们引入了尾部合并策略，将非全二叉树的尾部节点与同一层的前节点合并。
如图~\autoref{fig:tail_merging_tail}所示，在第一轮哈希中，我们将第7个节点和第6个节点合并，以尽可能保持数据的相似性。
数据6-7的哈希结果是\texttt{efcbeb}，数据5-6的哈希值是\texttt{efcbef}。
显然，在第一轮哈希中存在很高的相似性，并且可以保持到下一级哈希。
这将传输的哈希值大小从12位减少到7位，代价是在第一轮哈希中只传输了1个不同的比特。
这样，在进行完整性证明时，我们只需要传输不同的哈希位，从而减少了传输的数据量。

\section{本章小结}
在本章中，为了在数据完整性验证的过程中减少数据传输延迟，我们用LSH树替代了原本的默克尔树结构。
LSH树利用了物联网场景中数据相似度极高的性质，使用LSH算法减少哈希后数据的差异位数，从而减少完整性验证过程中需要传输的数据量。
我们分析了LSH树的安全风险，并且针对非满二叉树的情况，提出了尾部归并的策略来保障LSH树层级间的相似性。

\chapter{实验结果与分析}
在本章中，我们首先从存储延迟和查询延迟方面评估了TimeChain的总体性能，证明TimeChain设计的有效性；
其次，在不同查询负载、不同存储网络的条件下分析了TimeChain的具体表现；
最后，我们针对提出的三个技术点进行了消融实验，对比技术点带来的性能提升。

\section{实验设置}
我们基于一些开源项目（如Hyperledger Fabric和IPFS）实现TimeChain。
我们使用阿里云的50台虚拟机构建了一个基于Hyperledger Fabric的底层区块链系统，每个节点配备2核CPU和4GB内存。
块大小设置为1500个交易，块间隔为1秒。
我们基于一个真实世界的存储网络~\cite{corneo2021surrounded}，模拟了分布在世界各地的320个云服务器节点，并基于该集群进行了实验。
每个存储节点配置2核CPU和4GB内存，每个存储节点的存储空间为512GB。
存储节点和网关之间的距离从800公里到6000公里不等，平均为4000公里。
我们使用现有的线性回归模型~\cite{ziviani2005improving}模拟存储节点和网关之间的传输延迟。
考虑到一些存储提供商存在欺诈行为，部分远程存储的数据将有概率无法被访问到，概率为60\%。
我们使用电脑（Personal Computer，PC）作为物联网传感器的网关节点，它配备了Intel(R)Core i7-13700K CPU@5.4GHz、32GB DRAM，并运行Ubuntu 22.04。
默认聚合存储单元大小和查询大小设置为20。

\subsection{基线}
\textbf{SEBDB}~\cite{zhu2019sebdb}是链上存储的典型代表。
它通过将所有数据存储在区块链上，并使用B+树创建时间戳和设备名称的快速索引实现了对链上区块的高效访问。
在数据验证方面，SEBDB使用传统的默克尔树进行数据验证。
默克尔树通过计算数据块的哈希值，并将这些哈希值逐层组织成树结构，来实现数据完整性验证。
尽管SEBDB不是专门为物联网数据设计的链下存储解决方案，但是因为它的数据验证机制类似于TimeChain，我们将它作为对比方案。
为了保证实验的公平性，我们调整了SEBDB来记录单个数据项的存储位置并将其上传到链中，并随机选择存储节点。
这些修改确保了实验的公平性，同时更好地反映了物联网数据的特点和需求。

\textbf{FileDES}~\cite{xu2024filedes}是一个基于文件的链下存储系统。
它通过在远程节点上存储文件并在链上记录文件的哈希值来实现文件的安全存储和可靠性。
当客户端需要搜索文件时，FileDES会遍历区块链上的所有块，以获取文件的存储位置。
在数据验证方面，FileDES也使用与SEBDB相同的默克尔树。
由于FileDES是为文件存储设计的，为了确保实验的公平性，我们将FileDES改造成为适用于物联网数据的链下存储系统。
对于到达网关的物联网数据，FileDES将这些数据以固定大小、按照到达网关的顺序进行打包，并将这些数据上传到远程存储节点。

\subsection{数据集和工作负载}

我们使用了以下三个数据集进行实验：

\textbf{港珠澳大桥（Bridge）}~\cite{zhang2023edge}:
Bridge数据集包含4M条数据记录，这些数据是在5天内从港珠澳大桥上的6种传感器采集的。
这6种传感器与桥梁健康监测有关，包括桥梁各个位置的振动加速度、挠度等，共计53个。
由于数据生成频率为10Hz，我们将平均数据查询范围设置为100。

\textbf{RT-IFTTT（RT）}~\cite{heo2017rt}:
RT数据集包含680K条数据记录。
这些数据是在10天内从10个真实传感器采集的值，包括温度、湿度、可见光和其他传感器。
数据以秒为单位收集，因此查询的平均范围设置为20。

\textbf{天气 (WX)}~\footnote{https://www.kaggle.com/selfishgene/historical-hourly-weather-data/}:
WX数据集包含1.5M条数据记录，其中包括来自五年内30多个城市的各种天气属性每小时测量的数据。
鉴于数据收集频率相对较低，间隔最多一小时，我们假设查询范围为10。

考虑到时间序列存储系统的存储特性~\cite{naqvi2017time}，我们将这些数据集的格式转换为 $<measurement,\\ field~name, field~value, timetamp>$。
$Measurement$ 类似于数据表，用于组织数据，例如“天气”；
$field~name$ 用于标识不同的字段，例如“温度”；
$field~value$存储的是实际数据，比如温度传感器的实际值；
$timestamp$是时间戳，记录了数据的时间信息。
考虑到数据集的特点，我们将这些数据的大小分别固定为8B、32B、8B、8B。
因此，一条数据总共占用的存储空间为56B。

\section{总体性能}

\subsection{存储延迟}
\begin{figure*}[t]
    \centering
	\begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{timechain/storage_rt_eval.pdf}
        \caption{存储延迟}
        \label{fig:storage_rt_eval}
    \end{minipage}
    \quad
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{timechain/query_rt_eval.pdf}
        \caption{查询延迟}
        \label{fig:query_rt_eval}
	\end{minipage}
\end{figure*}
我们比较了不同聚合单元大小下的存储延迟。
由于物联网设备数量众多，数据生成速度快，我们将存储延迟定义为存储10000个数据的总延迟，而不是关注单个数据的微小延迟。
如~\autoref{fig:storage_rt_eval}所示，对于不同的聚合单元大小，TimeChain的存储延迟低于SEBDB和FileDES。
这是因为TimeChain的独特打包机制和节点选择机制减少了数据传输延迟。
此外，随着聚合单元大小变大，存储延迟变小。
这是因为，对于相同数量的数据，当聚合单元较大时，数据在链中打包和记录的次数会减少。
用户可以通过调整聚合单元大小来控制存储延迟。

\subsection{查询延迟}
我们比较了不同聚合单元大小的查询延迟，如~\autoref{fig:query_rt_eval}所示。
查询延迟是指基于固定查询范围随机查询数据集的平均延迟。
从~\autoref{fig:query_rt_eval}可以看出，TimeChain的查询延迟低于其他两种方案。
这是由于TimeChain减少了数据传输延迟，原因将在~\autoref{fig:query_breakdown}中解释。
我们还可以发现，当聚合单元大小增加时，查询延迟会减少。
这是因为当聚合单元大小增加时，同一查询中涉及的聚合单元数量会减少，用户的查询结果将试图集中在一个存储节点上。
然而，聚合单元越大，TimeChain相对于其他解决方案的改进将随着聚合单元的增加而减少。
这是因为当聚合单元非常大时，相当于将所有数据存储在一个聚合单元中，在这种情况下，数据批处理不会导致性能提高。
此外，当大量数据集中在存储节点中时，存储系统的可扩展性和可靠性也会受到损害。

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{timechain/query_breakdown.pdf}
        \caption{查询延迟分解}
        \label{fig:query_breakdown}
	\end{minipage}
	\quad
	\begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{timechain/support_device.pdf}
        \caption{最大支持存储设备数}
        \label{fig:support_device}
    \end{minipage}
\end{figure*}

\subsection{查询延迟分解}
我们进一步分析了查询延迟的细分，如~\autoref{fig:query_breakdown}所示。
查询的延迟主要由4个阶段组成，检索、传输、验证和返回。
考虑到传感器通常选择更近的网关，返回阶段的延迟几乎可以忽略不计。
在验证阶段，三种方案的延迟相对接近，小于1ms，几乎可以忽略不计。
传输和检索阶段的延迟是查询延迟的主要部分。
TimeChain的传输延迟明显低于FileDES和SEBDB。
这是因为TimeChain独特的节点打包机制和选择机制，减少了数据获取次数，缩短了与存储提供商的距离。
对于检索阶段，由于FileDES遍历所有块来检索数据，因此检索延迟特别高。
而SEBDB和TimeChain分别使用B+和R树分别构建索引，从而将延迟降低到1ms以下。

\subsection{支持的最大存储设备数}
具体来说，我们使用支持设备的最大数量指标，即存储系统每秒可以支持存储服务的设备数量。
我们假设网关可以处理来自多个物联网设备的数据存储请求，并忽略网关本身的处理延迟。
所有物联网设备同时以1Hz的频率生成数据，并要求在生成下一个数据之前必须存储这些数据。
如~\autoref{fig:support_device}所示，TimeChain与SEBDB和FileDES相比，支持的最大设备数量分别增加了1.63倍和3.55倍。
这主要是由于TimeChain的快速存储延迟，其中数据传输延迟非常低，允许TimeChain以更快的速度存储数据。
此外，支持的设备的最大数量将随着聚合单元大小的增加而增加。
当聚合单元大小达到80时，TimeChain支持的最大设备数已达到千级。

\begin{figure}[t]
    \centering
	\begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{timechain/query_diff_dataset.pdf}
        \caption{不同查询大小下的查询延迟}
        \label{fig:query_diff_dataset}
	\end{minipage}
	\quad
	\begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{timechain/storage_diff_storage_nodes.pdf}
        \caption{不同存储网络下的存储延迟}
        \label{fig:storage_diff_storage_nodes}
    \end{minipage}
\end{figure}

\section{性能影响因素评估}
\subsection{不同查询大小下的查询延迟}
我们比较了三种不同查询大小数据集（Bridge、RT和WX）的查询性能，结果如~\autoref{fig:query_diff_dataset}所示。
三个数据集中工作负载的平均查询大小分别为100、20和10。
我们可以发现，这三种解决方案的查询延迟通常随着查询大小的增加而降低。
这可以归因于这样一个事实，即更大的查询大小意味着查询中覆盖了更多的数据，从而提高了数据局部性和查询效率。
我们观察到，与SEBDB和FileDES相比，当查询大小变大时，TimeChain带来的性能改进也会增加。
这是因为当查询大小变大时，SEBDB和FileDES通常需要从比TimeChain更多的节点获取数据。

\subsection{存储网络规模下的存储延迟}
我们比较了不同存储网络规模下每种解决方案的存储延迟，如~\autoref{fig:storage_diff_storage_nodes}所示。
随着存储节点数量的增加，TimeChain的存储延迟呈下降趋势。
这是因为当节点数量增加时，TimeChain中的网关可以选择更多的存储提供程序，这增加了选择更近节点的可能性。
其他解决方案将不会从存储节点数量的增加中受益。
这是因为FileDES和SEBDB都随机选择存储节点，而不断增长的存储节点不会显著影响随机选择的结果。
因此，FileDES和SEBDB的存储延迟显示出相对较大的随机性。

\section{消融实验}
在本小节中，我们通过三项消融研究证明了我们的设计的改进效果。

\subsection{聚类算法}
\begin{figure*}[t]
    \centering
	\begin{minipage}{0.96\linewidth}
        \subfloat[查询延迟]{
            \centering
            \includegraphics[width=0.45\textwidth]{timechain/ratiocut_latency_eval.pdf}
            \label{fig:ratiocut_latency_eval}
        }
        \subfloat[跨越的batch]{
            \centering
            \includegraphics[width=0.47\textwidth]{timechain/ratiocut_batch_eval.pdf}
            \label{fig:ratiocut_batch_eval}
        }
        \caption{聚类算法消融实验} 
    \end{minipage}
\end{figure*}

我们比较了聚类算法的效果，并将TimeChain与SEBDB进行了比较。
如~\autoref{fig:ratiocut_latency_eval}所示，与SEBDB相比，TimeChain的网络传输延迟减少了40.3\%。
SEBDB中的打包节点没有考虑用户查询的规律性，聚合数据也没有根据数据源的类型进行划分。
当数据拥有者请求一系列数据时，SEBDB网关需要跨越来自更多存储提供商的多个聚合单元。
TimeChain使用谱聚类算法根据用户请求的特征对来自特定传感器的数据进行打包，导致访问聚合单元比SEBDB少59.3\%，如~\autoref{fig:ratiocut_batch_eval}所示。

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.96\linewidth}
        \vspace{0.5ex}
        \subfloat[查询延迟]{
            \centering
            \includegraphics[width=0.45\textwidth]{timechain/consensus_latency_eval.pdf}
            \label{fig:consensus_latency_eval}
        }
        \subfloat[存储服务质量]{
            \centering
            \includegraphics[width=0.47\textwidth]{timechain/consensus_prob_eval.pdf}
            \label{fig:consensus_prob_eval}
        }
        \caption{节点选择消融实验} 
    \end{minipage}
\end{figure*}

\subsection{节点选择}
我们在~\autoref{fig:consensus_latency_eval}中显示了TimeChain、FileDES和CRUSH在节点选择方面的性能差异。
FileDES~\cite{xu2024filedes}根据节点信誉划分一些受信任的存储节点，并从节点集中随机选择节点来存储数据。
因此，FileDES在三者中具有最高的存储服务提供概率，如~\autoref{fig:consensus_prob_eval}所示。
然而，FileDES的随机节点选择可能会引入更远的存储节点，从而导致更长的传输延迟。
CRUSH~\cite{weil2006ceph}根据节点的物理位置选择最近的节点进行存储，但不考虑存储节点故障和单点故障。
这使得CRUSH可以选择更接近但不可靠的存储节点，因此CRUSH选择的41\%的节点无法提供有效的存储服务。
另一方面，TimeChain考虑了节点的物理距离和节点信誉，在响应时间和服务概率方面都具有最佳性能。
虽然TimeChain选择的节点距离不是最接近的，但考虑到节点距离和服务质量，TimeChain的效果最好。

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.96\linewidth}
        \vspace{-0.5ex}
        \subfloat[查询延迟]{
            \centering
            \includegraphics[width=0.45\textwidth]{timechain/lsh_latency_eval.pdf}
            \label{fig:lsh_latency_eval}
        }
        \subfloat[完整性证明大小]{
            \centering
            \includegraphics[width=0.47\textwidth]{timechain/lsh_size_eval.pdf}
            \label{fig:lsh_size_eval}
        }
        \caption{LSH树消融实验} 
    \end{minipage}
\end{figure*}

\subsection{LSH树}
我们比较了FileDES和TimeChain的网络传输延迟，如~\autoref{fig:lsh_latency_eval}所示。
与FileDES相比，TimeChain的数据传输延迟减少了10.9\%。
这是由于当局域网中有更多的传感器设备并且数据生成频率很高时，数据传输量会显著影响拥塞网络中的传输延迟。
如~\autoref{fig:lsh_size_eval}所示，由于TimeChain使用LSH作为哈希算法，通过网络传输的数据量大大减少。
这将减少存储和查询延迟，并大大减轻存储提供商的存储负担。

\section{本章小结}
在本章中，我们基于开源组件实现了TimeChain，并在模拟的存储网络上进行了评估。
实验结果表明，TimeChain相比于现有的基于区块链的存储系统，平均减少了64.6\%的查询延迟和35.3\%的存储延迟，证明了其在存储性能方面的显著优势。
我们还通过消融实验验证了TimeChain的三个关键技术点的有效性。

\chapter{总结与展望}

\section{总结研究}
本文提出了TimeChain，一个基于区块链的物联网时序数据高效可信存储系统。针对物联网场景下数据生成速度快、数据量大、需要高效率和高可信度的数据存储问题，TimeChain仅将每批数据的哈希值存储在链上，而完整的原始数据则保存在链外。本文的主要创新点总结如下：

1. 提出了一种面向物联网数据的基本处理方案，即把每批数据的哈希值存储在链上以减少数据开销并提高存储效率。本文也对这个基本方案进行了一系列测量，发现了基本方案性能较差的核心原因，为后续的优化提供了方向。

2. 设计了一种自适应聚合机制，通过构建基于历史查询的无向加权图，将数据打包问题转化为图划分问题，并使用谱聚类算法优化数据的批处理存储，以减少聚合查询期间的节点访问次数。

3. 提出了一种基于共识的存储节点选择机制，结合存储节点信誉和传输距离，通过将共识过程与节点选择相结合，以减少传播延迟并提高节点选择的安全性。

4. 创新性地设计了基于LSH树的数据完整性验证机制，利用物联网数据的局部相似性，仅传输证明的非冗余部分，显著减少了完整性验证所需的数据量。

实现了TimeChain系统，并在开源组件（如Hyperledger Fabric和IPFS）上进行了评估。
实验结果表明，TimeChain相比于现有的基于区块链的存储系统，平均减少了64.6\%的查询延迟和35.3\%的存储延迟，证明了其在存储性能方面的显著优势。

\section{未来工作}
在未来的工作中，我们计划针对TimeChain系统进行以下几方面的研究和改进，以进一步提升系统性能并适应不同的应用场景。
1. 我们将在真实的环境中实现并部署TimeChain，以验证其在实际应用中的性能和可靠性，确保在实际部署中能够满足各种应用场景的需求。
这包括在不同的网络条件下，对算法的响应时间和吞吐量进行评估。

2. 我们计划探索轻量化或异步共识算法，如Raft和Tendermint，这些算法以其高效性而闻名。
这些算法能够在不牺牲安全性的前提下，减少通信开销，提高系统的响应速度和吞吐量，特别适合资源受限的物联网设备。

3. 在引入新的共识算法的同时，我们也将重点关注系统的安全性和稳定性。这包括对新算法的安全性进行形式化验证，以及在实际部署中对潜在的安全威胁进行防御。